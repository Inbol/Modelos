{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjO0SnEh3Wtu",
        "outputId": "04c0fccd-2a96-4575-f6eb-6b4e10f22b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.feature_selection import SelectFromModel, SelectPercentile, mutual_info_regression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "!pip install catboost\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from joblib import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y_true, y_pred): return root_mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Tuned params\n",
        "XGB_TUNED = {\n",
        "    \"n_estimators\": 1491, \"learning_rate\": 0.07704671891646622, \"max_depth\": 4,\n",
        "    \"min_child_weight\": 2, \"subsample\": 0.8545641645055122, \"colsample_bytree\": 0.7024273291045295,\n",
        "    \"reg_lambda\": 0.01286124731516405, \"reg_alpha\": 0.00034630370261191863,\n",
        "    \"gamma\": 2.1060501876471487e-08, \"objective\": \"reg:squarederror\", \"tree_method\": \"hist\",\n",
        "    \"random_state\": 42, \"n_jobs\": -1,\n",
        "}\n",
        "HUBER_TUNED = {\"alpha\": 2e-6, \"epsilon\": 2.58, \"max_iter\": 5000, \"tol\": 1e-4, \"fit_intercept\": True}\n",
        "CAT_TUNED = {\n",
        "    \"subsample\": 0.7, \"rsm\": 0.9, \"random_strength\": 1.0, \"n_estimators\": 1500,\n",
        "    \"learning_rate\": 0.032199564518489585, \"l2_leaf_reg\": 0.8733261623828433, \"depth\": 6,\n",
        "    \"bootstrap_type\": \"Bernoulli\", \"loss_function\": \"RMSE\", \"random_state\": 42,\n",
        "    \"verbose\": 0, \"allow_writing_files\": False,\n",
        "}\n",
        "LGB_TUNED = {\n",
        "    \"n_estimators\": 979, \"learning_rate\": 0.028251215820188597, \"num_leaves\": 48, \"max_depth\": 6,\n",
        "    \"min_child_samples\": 28, \"subsample\": 0.97658592351061, \"subsample_freq\": 3,\n",
        "    \"colsample_bytree\": 0.513255655270811, \"reg_alpha\": 0.00013313002526446513,\n",
        "    \"reg_lambda\": 0.0008138626231308923, \"min_split_gain\": 0.01243125804272925,\n",
        "    \"objective\": \"regression\", \"random_state\": 42, \"n_jobs\": -1, \"verbosity\": -1,\n",
        "}\n",
        "\n",
        "# Metrics\n",
        "def mdape(y_true, y_pred, eps: float = 1e-8) -> float:\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return float(np.median(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "def metrics_report(name, y_true, y_pred):\n",
        "    print(f\"{name:28s} | RMSE: {rmse(y_true, y_pred):,.2f} | \"\n",
        "          f\"MAE: {mean_absolute_error(y_true, y_pred):,.2f} | \"\n",
        "          f\"MdAPE: {mdape(y_true, y_pred):,.2f}% | R²: {r2_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# Drop high missing values columns\n",
        "def drop_high_missing(frame: pd.DataFrame, threshold=0.40):\n",
        "    miss_ratio = frame.isna().mean()\n",
        "    drop_cols = miss_ratio[miss_ratio > threshold].index.tolist()\n",
        "    return frame.drop(columns=drop_cols, errors='ignore'), drop_cols\n",
        "\n",
        "# Ordinal map\n",
        "ORDINAL_MAP = {\n",
        "    'ExterQual':   {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'ExterCond':   {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'BsmtQual':    {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'BsmtCond':    {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'HeatingQC':   {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'KitchenQual': {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'GarageQual':  {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'GarageCond':  {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n",
        "    'BsmtExposure':{'None':0,'No':1,'Mn':2,'Av':3,'Gd':4},\n",
        "    'BsmtFinType1':{'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6},\n",
        "    'BsmtFinType2':{'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6},\n",
        "    'LandSlope':   {'Gtl':0,'Mod':1,'Sev':2},\n",
        "    'LotShape':    {'Reg':3,'IR1':2,'IR2':1,'IR3':0},\n",
        "    'PavedDrive':  {'N':0,'P':1,'Y':2},\n",
        "    'Functional':  {'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7},\n",
        "    'Utilities':   {'ELO':0,'NoSeWa':1,'NoSewr':2,'AllPub':3},\n",
        "}\n",
        "\n",
        "def apply_ordinal_maps(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col, mapping in ORDINAL_MAP.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('object').fillna('None').map(mapping).astype('float64')\n",
        "    return df\n",
        "\n",
        "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, q_low=0.005, q_high=0.995):\n",
        "        self.q_low = q_low; self.q_high = q_high; self.bounds_ = None\n",
        "    def fit(self, X, y=None):\n",
        "        X = np.asarray(X, float)\n",
        "        lo = np.nanquantile(X, self.q_low, axis=0)\n",
        "        hi = np.nanquantile(X, self.q_high, axis=0)\n",
        "        self.bounds_ = (lo, np.maximum(hi, lo))\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X = np.asarray(X, float)\n",
        "        lo, hi = self.bounds_\n",
        "        return np.clip(X, lo, hi)\n",
        "\n",
        "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, min_freq=12, other_label=\"Other\"):\n",
        "        self.min_freq = min_freq; self.other_label = other_label; self.keep_levels_ = {}\n",
        "    def fit(self, X, y=None):\n",
        "        X_ = pd.DataFrame(X).apply(lambda s: s.astype('object'))\n",
        "        for c in X_.columns:\n",
        "            vc = X_[c].value_counts(dropna=False)\n",
        "            self.keep_levels_[c] = set(vc[vc >= self.min_freq].index.astype('object'))\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_ = pd.DataFrame(X).apply(lambda s: s.astype('object').fillna('None'))\n",
        "        for c in X_.columns:\n",
        "            keep = self.keep_levels_.get(c, None)\n",
        "            if keep is not None:\n",
        "                X_.loc[~X_[c].isin(keep), c] = self.other_label\n",
        "        return X_\n",
        "\n"
      ],
      "metadata": {
        "id": "jssyjFZq3hit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessors\n",
        "def make_preprocessor_tree():\n",
        "    # column selectors\n",
        "    num_sel = selector(dtype_include=[np.number])\n",
        "    cat_sel = selector(dtype_include=['object','string','category','bool'])\n",
        "\n",
        "    numeric_pipe = Pipeline(steps=[\n",
        "        ('clip',    QuantileClipper(0.005, 0.995)),\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "    ])\n",
        "\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    categorical_pipe = Pipeline(steps=[\n",
        "        ('rare',    RareCategoryGrouper(min_freq=12, other_label='Other')),\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe',     ohe),\n",
        "    ])\n",
        "\n",
        "    return Pipeline(steps=[\n",
        "        ('cols', ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_pipe, num_sel),\n",
        "                ('cat', categorical_pipe, cat_sel),\n",
        "            ],\n",
        "            verbose_feature_names_out=False\n",
        "        )),\n",
        "    ])\n",
        "\n",
        "def make_preprocessor_linear():\n",
        "    # column selectors\n",
        "    num_sel = selector(dtype_include=[np.number])\n",
        "    cat_sel = selector(dtype_include=['object','string','category','bool'])\n",
        "\n",
        "    numeric_pipe = Pipeline(steps=[\n",
        "        ('clip',    QuantileClipper(0.005, 0.995)),\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler',  StandardScaler(with_mean=True, with_std=True)),\n",
        "    ])\n",
        "\n",
        "    categorical_pipe = Pipeline(steps=[\n",
        "        ('rare',    RareCategoryGrouper(min_freq=12, other_label='Other')),\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
        "    ])\n",
        "\n",
        "    return Pipeline(steps=[\n",
        "        ('cols', ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_pipe, num_sel),\n",
        "                ('cat', categorical_pipe, cat_sel),\n",
        "            ],\n",
        "            remainder='drop',\n",
        "            verbose_feature_names_out=False\n",
        "        )),\n",
        "    ])\n",
        "\n",
        "def score_func_42(X, y):\n",
        "    return mutual_info_regression(X, y, random_state=42)\n",
        "# Pipelines\n",
        "def make_pipeline_xgb(selector_threshold='0.75*median', xgb_kwargs=None):\n",
        "    steps = [\n",
        "        ('prep',   make_preprocessor_tree()),\n",
        "        ('filter_mi', SelectPercentile(score_func=score_func_42, percentile=80)),\n",
        "        ('select', SelectFromModel(\n",
        "            estimator=XGBRegressor(**xgb_kwargs),\n",
        "            threshold=selector_threshold,\n",
        "            importance_getter='feature_importances_'\n",
        "        )),\n",
        "        ('reg', XGBRegressor(**xgb_kwargs)),\n",
        "    ]\n",
        "    return Pipeline(steps)\n",
        "\n",
        "def make_pipeline_huber(huber_kwargs):\n",
        "    steps = [\n",
        "        ('prep', make_preprocessor_linear()),\n",
        "        ('filter_mi',  SelectPercentile(score_func=score_func_42, percentile=85)),\n",
        "        ('reg',  HuberRegressor(**huber_kwargs)),\n",
        "    ]\n",
        "    return TransformedTargetRegressor(regressor=Pipeline(steps), func=np.log1p, inverse_func=np.expm1)\n",
        "\n",
        "def make_pipeline_cat(selector_threshold='0.75*median', cat_kwargs=None):\n",
        "    steps = [\n",
        "        ('prep',   make_preprocessor_tree()),\n",
        "        ('filter_mi',  SelectPercentile(score_func=score_func_42, percentile=85)),\n",
        "        ('select', SelectFromModel(\n",
        "            estimator=ExtraTreesRegressor(\n",
        "                n_estimators=800,\n",
        "                max_depth=8,\n",
        "                min_samples_leaf=10,\n",
        "                min_samples_split=20,\n",
        "                max_features=0.5,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            threshold=selector_threshold, importance_getter='feature_importances_'\n",
        "        )),\n",
        "        ('reg',    CatBoostRegressor(**cat_kwargs)),\n",
        "    ]\n",
        "    return Pipeline(steps)\n",
        "\n",
        "def make_pipeline_lgb(selector_threshold='0.75*median', lgb_kwargs=None):\n",
        "    steps = [\n",
        "        ('prep',   make_preprocessor_tree()),\n",
        "        ('filter_mi',  SelectPercentile(score_func=score_func_42, percentile=80)),\n",
        "        ('select', SelectFromModel(\n",
        "            estimator=ExtraTreesRegressor(\n",
        "                n_estimators=800,\n",
        "                max_depth=8,\n",
        "                min_samples_leaf=10,\n",
        "                min_samples_split=20,\n",
        "                max_features=0.5,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            threshold=selector_threshold, importance_getter='feature_importances_'\n",
        "        )),\n",
        "        ('reg',    LGBMRegressor(**lgb_kwargs)),\n",
        "    ]\n",
        "    return Pipeline(steps)"
      ],
      "metadata": {
        "id": "-0CgT82K9zh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV / Utils\n",
        "def make_stratified_folds(y, n_splits=5, n_repeats=1, random_state=42, n_bins=10):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    qs = np.linspace(0, 1, n_bins + 1)\n",
        "    bins = np.unique(np.quantile(y, qs))\n",
        "    yb = np.digitize(y, bins[1:-1], right=True)\n",
        "    folds = []\n",
        "    for _ in range(n_repeats):\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=rng.randint(0, 10**6))\n",
        "        for tr, va in skf.split(np.zeros_like(yb), yb):\n",
        "            folds.append((tr, va))\n",
        "    return folds\n",
        "\n",
        "def find_best_weights_grid4(y, a, b, c, d, metric=\"rmse\", grid=41):\n",
        "    y = np.asarray(y, float)\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    c = np.asarray(c, float); d = np.asarray(d, float)\n",
        "    ws = np.linspace(0.0, 1.0, grid)\n",
        "    best = (0.25, 0.25, 0.25, 0.25); best_score = np.inf\n",
        "    for wa in ws:\n",
        "        rem1 = 1.0 - wa\n",
        "        for wb in ws:\n",
        "            if wb > rem1: break\n",
        "            rem2 = rem1 - wb\n",
        "            for wc in ws:\n",
        "                if wc > rem2: break\n",
        "                wd = 1.0 - wa - wb - wc\n",
        "                if wd < 0: continue\n",
        "                yhat = wa*a + wb*b + wc*c + wd*d\n",
        "                sc = rmse(y, yhat)\n",
        "                if sc < best_score:\n",
        "                    best_score = sc\n",
        "                    best = (float(wa), float(wb), float(wc), float(wd))\n",
        "    return best, float(best_score)\n",
        "\n",
        "# SegBlend wrapper\n",
        "class WeightedSegmentedBlend4:\n",
        "    def __init__(self, xgb_pipe, hub_pipe, cat_pipe, lgb_pipe, edges, weights_per_bin):\n",
        "        self.xgb = xgb_pipe; self.hub = hub_pipe; self.cat = cat_pipe; self.lgb = lgb_pipe\n",
        "        self.edges = np.asarray(edges, dtype=float)\n",
        "        self.w_per_bin = [tuple(map(float, w)) for w in weights_per_bin]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.xgb.fit(X, y); self.hub.fit(X, y); self.cat.fit(X, y); self.lgb.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        px = self.xgb.predict(X)\n",
        "        ph = self.hub.predict(X)\n",
        "        pc = self.cat.predict(X)\n",
        "        pl = self.lgb.predict(X)\n",
        "\n",
        "        proxy = np.median(np.vstack([px, ph, pc, pl]), axis=0)\n",
        "        bins_test = np.digitize(proxy, self.edges[1:-1], right=True)\n",
        "\n",
        "        wx = np.array([self.w_per_bin[b][0] for b in bins_test], float)\n",
        "        wh = np.array([self.w_per_bin[b][1] for b in bins_test], float)\n",
        "        wc = np.array([self.w_per_bin[b][2] for b in bins_test], float)\n",
        "        wl = np.array([self.w_per_bin[b][3] for b in bins_test], float)\n",
        "        return wx * px + wh * ph + wc * pc + wl * pl\n"
      ],
      "metadata": {
        "id": "oKPZvpuY96Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "def main():\n",
        "    df = pd.read_csv('housing_train.csv')\n",
        "    y = df['SalePrice'].astype(float)\n",
        "\n",
        "    X_all = df.drop(columns=['SalePrice', 'Id'], errors='ignore')\n",
        "    X = X_all\n",
        "    X, dropped_missing = drop_high_missing(X, threshold=0.40)\n",
        "    X = apply_ordinal_maps(X)\n",
        "    print(f\"Columns dropped (>40% missing): {dropped_missing}\")\n",
        "\n",
        "    # Holdout split\n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "    # Base models\n",
        "    xgb = make_pipeline_xgb(selector_threshold='0.75*median', xgb_kwargs=XGB_TUNED)\n",
        "    hub = make_pipeline_huber(huber_kwargs=HUBER_TUNED)\n",
        "    cat = make_pipeline_cat(selector_threshold='0.75*median', cat_kwargs=CAT_TUNED)\n",
        "    lgb = make_pipeline_lgb(selector_threshold='0.75*median', lgb_kwargs=LGB_TUNED)\n",
        "\n",
        "    # OOF\n",
        "    folds = make_stratified_folds(y_tr, n_splits=5, n_repeats=1, random_state=42, n_bins=10)\n",
        "    oof_x = np.empty(len(y_tr), dtype=float); oof_x[:] = np.nan\n",
        "    oof_h = np.empty(len(y_tr), dtype=float); oof_h[:] = np.nan\n",
        "    oof_c = np.empty(len(y_tr), dtype=float); oof_c[:] = np.nan\n",
        "    oof_l = np.empty(len(y_tr), dtype=float); oof_l[:] = np.nan\n",
        "\n",
        "    for tr_idx, va_idx in folds:\n",
        "        X_tr_i, X_va_i = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
        "        y_tr_i = y_tr.iloc[tr_idx]\n",
        "\n",
        "        x_i = make_pipeline_xgb(selector_threshold='0.75*median', xgb_kwargs=XGB_TUNED)\n",
        "        h_i = make_pipeline_huber(huber_kwargs=HUBER_TUNED)\n",
        "        c_i = make_pipeline_cat(selector_threshold='0.75*median', cat_kwargs=CAT_TUNED)\n",
        "        l_i = make_pipeline_lgb(selector_threshold='0.75*median', lgb_kwargs=LGB_TUNED)\n",
        "\n",
        "        oof_x[va_idx] = x_i.fit(X_tr_i, y_tr_i).predict(X_va_i)\n",
        "        oof_h[va_idx] = h_i.fit(X_tr_i, y_tr_i).predict(X_va_i)\n",
        "        oof_c[va_idx] = c_i.fit(X_tr_i, y_tr_i).predict(X_va_i)\n",
        "        oof_l[va_idx] = l_i.fit(X_tr_i, y_tr_i).predict(X_va_i)\n",
        "\n",
        "    # Segment edges + per-bin weights\n",
        "    q_edges = np.quantile(y_tr, [0.0, 0.33, 0.66, 1.0])\n",
        "    bins_tr = np.digitize(y_tr, q_edges[1:-1], right=True)\n",
        "\n",
        "    w_per_bin = []\n",
        "    for b in range(3):\n",
        "        m = (bins_tr == b)\n",
        "        (wx, wh, wc, wl), _ = find_best_weights_grid4(\n",
        "            y_tr.values[m], oof_x[m], oof_h[m], oof_c[m], oof_l[m],\n",
        "            metric=\"rmse\", grid=41\n",
        "        )\n",
        "        w_per_bin.append((wx, wh, wc, wl))\n",
        "    print(\"[Segmented-4] weights per bin (low, mid, high):\",\n",
        "          [tuple(round(x, 3) for x in w) for w in w_per_bin])\n",
        "    print()\n",
        "    # Holdout training & report\n",
        "    xgb.fit(X_tr, y_tr); hub.fit(X_tr, y_tr); cat.fit(X_tr, y_tr); lgb.fit(X_tr, y_tr)\n",
        "    metrics_report(\"XGBoost\",  y_va, xgb.predict(X_va))\n",
        "    metrics_report(\"Huber\",    y_va, hub.predict(X_va))\n",
        "    metrics_report(\"CatBoost\", y_va, cat.predict(X_va))\n",
        "    metrics_report(\"LightGBM\", y_va, lgb.predict(X_va))\n",
        "    seg4 = WeightedSegmentedBlend4(xgb, hub, cat, lgb, q_edges, w_per_bin)\n",
        "    pred_seg4 = seg4.predict(X_va)\n",
        "    metrics_report(\"Segmented Blend x4\", y_va, pred_seg4)\n",
        "    print()\n",
        "\n",
        "    # Refit on ALL data\n",
        "    make_pipeline = lambda: (\n",
        "        make_pipeline_xgb(selector_threshold='0.75*median', xgb_kwargs=XGB_TUNED),\n",
        "        make_pipeline_huber(huber_kwargs=HUBER_TUNED),\n",
        "        make_pipeline_cat(selector_threshold='0.75*median', cat_kwargs=CAT_TUNED),\n",
        "        make_pipeline_lgb(selector_threshold='0.75*median', lgb_kwargs=LGB_TUNED),\n",
        "    )\n",
        "    xgb_full, hub_full, cat_full, lgb_full = make_pipeline()\n",
        "    xgb_full.fit(X, y); hub_full.fit(X, y); cat_full.fit(X, y); lgb_full.fit(X, y)\n",
        "\n",
        "    # Build a blended model using the full-data refits\n",
        "    seg4_full = WeightedSegmentedBlend4(\n",
        "        xgb_pipe=xgb_full,\n",
        "        hub_pipe=hub_full,\n",
        "        cat_pipe=cat_full,\n",
        "        lgb_pipe=lgb_full,\n",
        "        edges=q_edges,\n",
        "        weights_per_bin=w_per_bin\n",
        "    )\n",
        "\n",
        "    # (Optional) attach a few helpful attributes for sanity checks later\n",
        "    seg4_full.feature_names_ = X.columns.tolist()\n",
        "    seg4_full.versions_ = {\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "    }\n",
        "\n",
        "    # Save as joblib (compression level 3 is a nice size/speed tradeoff)\n",
        "    #dump(seg4_full, \"XGBxHUBxCBxLGBM_MiniModel_OvrTop20.joblib\", compress=3)\n",
        "    #print(\"Saved XGBxHUBxCBxLGBM.joblib\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs2LV-5199DQ",
        "outputId": "4482dbb0-cbdf-4040-eaf6-faec76563b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns dropped (>40% missing): ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Segmented-4] weights per bin (low, mid, high): [(0.0, 0.5, 0.5, 0.0), (0.0, 0.0, 1.0, 0.0), (0.375, 0.625, 0.0, 0.0)]\n",
            "\n",
            "XGBoost                      | RMSE: 25,256.61 | MAE: 16,162.49 | MdAPE: 6.37% | R²: 0.9168\n",
            "Huber                        | RMSE: 20,789.04 | MAE: 13,626.45 | MdAPE: 5.58% | R²: 0.9437\n",
            "CatBoost                     | RMSE: 25,529.03 | MAE: 15,434.66 | MdAPE: 5.50% | R²: 0.9150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM                     | RMSE: 27,525.03 | MAE: 15,996.62 | MdAPE: 5.57% | R²: 0.9012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Blend x4           | RMSE: 20,949.73 | MAE: 13,196.60 | MdAPE: 4.73% | R²: 0.9428\n",
            "\n"
          ]
        }
      ]
    }
  ]
}